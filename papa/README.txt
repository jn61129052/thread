写代码的过程中发现了如下几个问题

1 代码需要重构，耦合性实在是太强了，后期代码需要加入很多的功能，所以需要有良好的可维护性
2 urllib2是否真的不好用，requests需要了解一下
3 多线程和线程池的学习
4 python class的学习
5 多线程抓取网页，目前想到两个问题：假如有个别网页读取时间过长，如何正确的调度线程？布隆过滤器判断url队列是否
被下载，对性能和准确度影响有多大？是否还有其他合适的算法？
6 chardet判断网页的正确编码时间过长，是否需要替换？如何提高性能？
果断抛弃速度慢的要死的chardet，投入我requests encoding的怀抱
7 跟同事讨论了一下
URL 去重：将URL设为数据库的唯一主键，如果有重复的URL，抛出异常不入库
多线程爬虫：将地址设为地址池，将URL的深度设为属性，多线程轮流去地址池内获取URL进行处理以后入库，
单线程运行时间为2.34秒